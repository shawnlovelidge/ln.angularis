## LLM Demonstration of Code Generation

### Prerequisites
Its important to setup the scenario for success. To do this, its important to create empty test modules; one for each code module; recursively for a give source and destination folder.

Example:
```sh
./scripts/.setup.test.cases.sh /Volumes/Development/ln.angularis/projects/core/src/lib /Volumes/Development/ln.angularis/projects/core/src/test
```

### LLM Narrowed Context
Most LLM's can be augmented via :

1. Fine‑Tuning the LLM with a Directory of Files
  (a) Prepare the training set
  (b) Injest the files into a Dataset
  (c) Fine-tune the model.
2. Retreival Augmented Generation (RAG)
  (a) Build a Retreival Pipeline
    i. Index Code Modules
      a. **Embeddings:** Generate Embeddings for each document using an embedding model and store them in a vector database (store like [Facebook AI Similarity Search - FAISS](https://github.com/facebookresearch/faiss))
    ii. **Retrieval and Prompt Augmentation:** When a query is received by your LLM, compute its embedding, use the FAISS index to retrieve the most relevant documents, and then prepend (or otherwise include) that text in the prompt you send to the LLM.
    iii. **Integrate into Your LLM Workflow**
    Adapt your LLM’s inference pipeline to run the retrieval step before generating an answer. This way, each query is answered with context drawn from your directory of files.

**NOTE**: Need to check LLM’s documentation to see if it provides built‑in support or recommended practices for ingesting a directory of files. The above examples are generic and can be adapted to your specific setup.




#### Example:

```sh
write Jest Test Case in Typescript with 100% coverage for the <<filename>> module
```