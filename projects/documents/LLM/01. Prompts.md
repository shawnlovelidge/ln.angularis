# Code Development (via Machine Learning)

## I Basic Demonstration

### A. Setup Test Files (optional)
Create empty test modules; one for each code module; recursively for a give DESTINATION directory using SOURCE folder as input.

Example:
```sh
./scripts/.setup.test.cases.sh /Volumes/Development/ln.angularis/projects/core/src/lib /Volumes/Development/ln.angularis/projects/core/src/test
```

### B. Retrival Augmented Gernation (RAG)
Retrival Augmented Gernation (RAG) Process - Give module (thus providing context) to LLM. Then generate a prompt to generate code snippet.

### C. Generate PROMPT
After the model has been augemented then the developer needs to create a PROMPT;

Example:

```text

Generate the Jest test case in TypeScript with 100% coverage
for the this module; making sure to test the initializer
for the constructor and only creating test cases for public modifiers. Output the results in a TypeScript spec file.


```

## II Production Ready

Need to train the LLM via contexting it context in order to have a greater chance of producing desired coding results. We must fine tune the model via giving it a directory of files (i.e. code modules). There are two augementation options...

### 1. Train LLM (more time intensive)
  (a) Prepare the training set
  (b) Injest the files into a Dataset
  (c) Fine-tune the model.

### 2. Retreival Augmented Generation (RAG) - Temporary Content
Build a Retreival Pipeline via

  **A. Creating Embeddings:** Generate Embeddings (vectors) for each document using an embedding model and store them in a vector database (store like [Facebook AI Similarity Search - FAISS](https://github.com/facebookresearch/faiss))

  **B. Retrieval and Prompt Augmentation:** When a query is received by your LLM, compute its embedding, use the FAISS index to retrieve the most relevant documents, and then prepend (or otherwise include) that text in the prompt you send to the LLM.

  **C. Integrate into Your LLM Workflow**
    Adapt your LLM’s inference pipeline to run the retrieval step before generating an answer. This way, each query is answered with context drawn from your directory of files.

**NOTE**: Need to check LLM’s documentation to see if it provides built‑in support or recommended practices for ingesting a directory of files. The above examples are generic and can be adapted to your specific setup.

